---
title: "Homework2"
author: "Anil Akyildirim"
date: "4/10/2020"
output: html_document
---

```{r}

library(class)
library(pROC)
library(ROCR)
library(caret)
library(C50)
library(naivebayes)
library(e1071)
library(naivebayes)
library(MASS)
library(mltools)


```




## Load Data

```{r}
# Create the Dataset
x <- c(5,5,5,5,5,5,19,19,19,19,19,19,35,35,35,35,35,35,51,51,51,51,51,51,55,55,55,55,55,55,63,63,63,63,63,63)
y <- c('a','b','c','d','e','f','a','b','c','d','e','f','a','b','c','d','e','f','a','b','c','d','e','f','a','b','c','d','e','f','a','b','c','d','e','f')
label <- c('BLUE',	'BLACK',	'BLUE',	'BLACK',	'BLACK',	'BLACK', 	'BLUE',  'BLUE',  'BLUE',	'BLUE',	'BLACK',	'BLUE',	'BLACK',	'BLACK',	'BLUE',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLUE',	'BLACK', 'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLACK',	'BLUE',	'BLUE',	'BLUE',	'BLUE',	'BLUE')


df <- data.frame(x,y,label)

df$y = as.numeric(df$y) 

#Blue is 1 and Black is 0
df$label <- ifelse(df$label=="BLUE", 1, 0)
#df$label =as.numeric(df$label)
df$label =as.factor(df$label)

```

## KNN 
```{r}

set.seed(12)

##create a random number equal 60% of total number of rows
ran <- sample(1:nrow(df),0.60 * nrow(df))
##the normalization function is created
#nor <-function(x) {(x -min(x))/(max(x)-min(x))}

#df_norm <- as.data.frame(lapply(iris[,c(1,2)], nor))

##training dataset extracted
df_train <- df[ran,]
 
##test dataset extracted
df_test <- df[-ran,]

#create model
pr_knn <- knn(df_train,df_test,cl=df_train$label,k=10, prob=TRUE)
confusionMatrix(pr_knn, df_test$label)

#find auc
df_test$label=as.numeric(df_test$label)
auc_roc(pr_knn, df_test$label)
```


## Tree

```{r}
#create model
df_test$label = as.factor(df_test$label)
set.seed(2)
pr_tree <- C5.0(df_train$label~., data=df_train)
#table(predict(pr_tree, newdata = df_test), df_test$label)
pred_tree <- predict(pr_tree, df_test, type="class")
confusionMatrix(pred_tree, df_test$label)
df_test$label=as.numeric(df_test$label)
auc_roc(pred_tree, df_test$label)

```




## LR

```{r}
#creating model
set.seed(123)
pr_lr <- glm(label~., data=df_train, family = binomial(link = "logit"))
pred_lr <-predict(pr_lr, newdata=df_test, type = "response")
df_test$label=as.numeric(df_test$label)
auc_roc(pred_lr, df_test$label)


```

## Naive Bayes

```{r}
#create a model
set.seed(1234)
pr_nb <- naive_bayes(label~., data=df_train)
pred_nb <- predict(pr_nb, df_test)
confusionMatrix(pred_nb, df_test$label)
df_test$label=as.numeric(df_test$label)
auc_roc(pred_nb, df_test$label)

```




## LDA

```{r}
#create a model
set.seed(123456)
pr_lda <- lda(label~., data=df_train)
pred_lda <- predict(pr_lda, df_test)

df_test$label=as.numeric(df_test$label)
auc_roc(pred_lda, df_test$label)
```

## SVM


```{r}
set.seed(12323)
pr_svm <- svm(label~., data=df_train, kernel="radial", cost=5, scale=F)
pred_svm <- predict(pr_svm, df_test)
confusionMatrix(pred_svm, df_test$label)

df_test$label=as.numeric(df_test$label)
auc_roc(pred_svm, df_test$label)

```

```{r}
ALGO <- c("KNN", "TREE", "NB", "SVM", "LR")
AUC <- c(0.525, 0.425, 0.625, 0.475, 0.67)
ACC <- c(0.60, 0.73, 0.33, 0.66, 0.67)
TPR <- c(0.9, 0.9, 0.5, 0.9, 0.9)
FPR <- c(1,0.6,1,0.8,1)

models <- data.frame(ALGO, AUC, ACC, TPR, FPR)
models


Overall Tree Model has the highest accuracy followed by SVM Radial Kernel. For KNN model, i didnt evaluate the best K neighbour but rather picked one, which is 10.  Unfortunately, I couldnt find the model evaluation metrics for LDA and Logistic Regression models. 
```


